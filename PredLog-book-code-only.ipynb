{
    "cells": [
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## Inital data analysis on log files"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Load the source file spp_log_192.168.99.81_20190919-133113.log"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "\nimport types\nimport pandas as pd\nfrom botocore.client import Config\nimport ibm_boto3\n\ndef __iter__(self): return 0\n\n# @hidden_cell\n# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n# You might want to remove those credentials before you share the notebook.\nclient_589fdcec84424efeafef706c303c6778 = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id='SNpSnb-1SxG2oqVa2zuBjtNVRlHFxUM_iNcwoMx7Ejpy',\n    ibm_auth_endpoint=\"https://iam.eu-de.bluemix.net/oidc/token\",\n    config=Config(signature_version='oauth'),\n    endpoint_url='https://s3.eu-geo.objectstorage.service.networklayer.com')\n\n# Your data file was loaded into a botocore.response.StreamingBody object.\n# Please read the documentation of ibm_boto3 and pandas to learn more about the possibilities to load the data.\n# ibm_boto3 documentation: https://ibm.github.io/ibm-cos-sdk-python/\n# pandas documentation: http://pandas.pydata.org/\nstreaming_body_1 = client_589fdcec84424efeafef706c303c6778.get_object(Bucket='predlog-donotdelete-pr-eptzlrr64e0mwe', Key='spp_log_192.168.99.81_20190919-133113.log')['Body']\n# add missing __iter__ method, so pandas accepts body as file-like object\nif not hasattr(streaming_body_1, \"__iter__\"): streaming_body_1.__iter__ = types.MethodType( __iter__, streaming_body_1 ) \n",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Read it into a pandas table"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "import io\n\ndf = pd.read_table(io.BytesIO(streaming_body_1.read()))",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "df.head()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "OK, right, seems like we got all json data in one row or something."
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Try instead to read the json. The out.json file contains valid json from my hard drive. I know it works."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "\nbody = client_589fdcec84424efeafef706c303c6778.get_object(Bucket='predlog-donotdelete-pr-eptzlrr64e0mwe',Key='out.json')['Body']\n# add missing __iter__ method, so pandas accepts body as file-like object \n\nif not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n\n# Since JSON data can be semi-structured and contain additional metadata, it is possible that you might face an error during data loading.\n# Please read the documentation of 'pandas.read_json()' and 'pandas.io.json.json_normalize' to learn more about the possibilities to adjust the data loading.\n# pandas documentation: http://pandas.pydata.org/pandas-docs/stable/io.html#io-json-reader\n# and http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.json.json_normalize.html\n\ndf_data_1 = pd.read_json(body, orient='values')\ndf_data_1.head()\n",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Let's apply the same code to our log file."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "\nbody = client_589fdcec84424efeafef706c303c6778.get_object(Bucket='predlog-donotdelete-pr-eptzlrr64e0mwe',Key='spp_log_192.168.99.81_20190919-133113.json')['Body']\n# add missing __iter__ method, so pandas accepts body as file-like object \n\nif not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n\n# Since JSON data can be semi-structured and contain additional metadata, it is possible that you might face an error during data loading.\n# Please read the documentation of 'pandas.read_json()' and 'pandas.io.json.json_normalize' to learn more about the possibilities to adjust the data loading.\n# pandas documentation: http://pandas.pydata.org/pandas-docs/stable/io.html#io-json-reader\n# and http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.json.json_normalize.html\n\ndf_data_2 = pd.read_json(body, orient='values')\ndf_data_2.head()\n",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Nope."
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# Try another log file in the set. Maybe the first one was corrupt."
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "This one spp_log_192.168.99.81_20190919-144518.log and spp_log_192.168.99.81_20190919-145051.log."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "\n# Your data file was loaded into a botocore.response.StreamingBody object.\n# Please read the documentation of ibm_boto3 and pandas to learn more about the possibilities to load the data.\n# ibm_boto3 documentation: https://ibm.github.io/ibm-cos-sdk-python/\n# pandas documentation: http://pandas.pydata.org/\nstreaming_body_2 = client_589fdcec84424efeafef706c303c6778.get_object(Bucket='predlog-donotdelete-pr-eptzlrr64e0mwe', Key='spp_log_192.168.99.81_20190919-144518.log')['Body']\n# add missing __iter__ method so pandas accepts body as file-like object\nif not hasattr(streaming_body_2, \"__iter__\"): streaming_body_2.__iter__ = types.MethodType( __iter__, streaming_body_2 ) \n",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "\n# Your data file was loaded into a botocore.response.StreamingBody object.\n# Please read the documentation of ibm_boto3 and pandas to learn more about the possibilities to load the data.\n# ibm_boto3 documentation: https://ibm.github.io/ibm-cos-sdk-python/\n# pandas documentation: http://pandas.pydata.org/\nstreaming_body_3 = client_589fdcec84424efeafef706c303c6778.get_object(Bucket='predlog-donotdelete-pr-eptzlrr64e0mwe', Key='spp_log_192.168.99.81_20190919-145051.log')['Body']\n# add missing __iter__ method so pandas accepts body as file-like object\nif not hasattr(streaming_body_3, \"__iter__\"): streaming_body_3.__iter__ = types.MethodType( __iter__, streaming_body_3 ) \n",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "df = pd.read_table(io.BytesIO(streaming_body_3.read()))",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "df.head(1)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Try to read the json out of it."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "json = df.read_json()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Bummer."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "df = pd.read_table(io.BytesIO(streaming_body_2.read()))",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "df.head()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "I went back to json formatter outside of this notebook to check the json. Seems something is not right.\n\n## Terminal commands to process files to be able to use them in jupyter notebook\n\nFound out I had to replace single quote with double quote.\n\ntr \"'\" '\"' <spp_log_192.168.99.81_20190919-133113.log > outfile.log\n\nNone was without double quotes\n\ncat outfile.log | sed 's/None/\\\"None\\\"/g' > outfile2.log\n  \nTrue was without double quotes\n\ncat outfile2.log | sed 's/True/\\\"True\\\"/g' > outfile3.log\n  \nFalse was without double quotes\n\ncat outfile3.log | sed 's/False/\\\"False\\\"/g' > outfile4.log\n    \n    "
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# The result was a preprocessed file called outfile4.log"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "\n# Your data file was loaded into a botocore.response.StreamingBody object.\n# Please read the documentation of ibm_boto3 and pandas to learn more about the possibilities to load the data.\n# ibm_boto3 documentation: https://ibm.github.io/ibm-cos-sdk-python/\n# pandas documentation: http://pandas.pydata.org/\nstreaming_body_4 = client_589fdcec84424efeafef706c303c6778.get_object(Bucket='predlog-donotdelete-pr-eptzlrr64e0mwe', Key='outfile4.log')['Body']\n# add missing __iter__ method so pandas accepts body as file-like object\nif not hasattr(streaming_body_4, \"__iter__\"): streaming_body_4.__iter__ = types.MethodType( __iter__, streaming_body_4 ) \n",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "import io\n\ndf = pd.read_table(io.BytesIO(streaming_body_4.read()))",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "df.head()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Hmm, better, and let's see what is inside. The formatter also gave me a clue to a section called 'alerts'"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# alerts is a list of alerts we want to convert to a table. \n## This is the columns (keys) with 2696 rows (values) of data:\n    \"links\": {\n        \"self\": {\n          \"rel\": \"self\",\n          \"href\": \"https://192.168.99.81/api/endeavour/alert/message/5c963e61e4b0f2f874f9ac27\",\n          \"hreflang\": \"None\",\n          \"media\": \"None\",\n          \"title\": \"None\",\n          \"type\": \"None\",\n          \"deprecation\": \"None\"\n        },\n        \"up\": {\n          \"rel\": \"up\",\n          \"href\": \"https://192.168.99.81/api/endeavour/alert/message\",\n          \"hreflang\": \"None\",\n          \"media\": \"None\",\n          \"title\": \"None\",\n          \"type\": \"None\",\n          \"deprecation\": \"None\"\n        }\n      },\n      \"name\": \"SQLLOG_ALERT_MESSAGE\",\n      \"alertTime\": 1553350241025,\n      \"category\": \"APPLICATION\",\n      \"categoryDisplayName\": \"APPLICATION\",\n      \"type\": \"ERROR\",\n      \"typeDisplayName\": \"ERROR\",\n      \"initialMessage\": \"Log backup failed for database [WideWorldImporters] on instance [SQL2016]. An exception occurred while executing a Transact-SQL statement or batch.. Error: 0x80131501\",\n      \"message\": \"Log backup failed for database [WideWorldImporters] on instance [SQL2016]. An exception occurred while executing a Transact-SQL statement or batch.. Error: 0x80131501\",\n      \"messageName\": \"None\",\n      \"messageParams\": \"None\",\n      \"initMessageParams\": \"None\",\n      \"dataSource\": \"agent_sql_log\",\n      \"status\": \"UNKNOWN\",\n      \"statusDisplayName\": \"UNKNOWN\",\n      \"storageId\": \"None\",\n      \"serverId\": \"None\",\n      \"jobId\": \"None\",\n      \"jobSessionId\": \"None\",\n      \"retention\": 180,\n      \"first\": 1553350241025,\n      \"last\": 1553350241025,\n      \"expiresAt\": 1568902241026,\n      \"count\": 1,\n      \"acknowledged\": \"True\",\n      \"expired\": \"False\",\n      \"unique\": \"False\",\n      \"id\": \"5c963e61e4b0f2f874f9ac27\""
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Another try with separator."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "df_extracted=pd.read_csv(io.BytesIO(streaming_body_4.read()), sep='\\t')",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Nope."
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Pseudo code"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "#extract alerts into rowList\n\n#iterate over rowList and do something\n#for row in rowList:\n#    print(row)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "df42 = pd.read_json(io.BytesIO(streaming_body_4.read()))",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "df.columns",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# Right, seems we are getting somewhere now. Could we reach the columns and how do they look like?"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "df.describe",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Try to grab the interesting column alerts."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "dfObj = pd.DataFrame(df, columns = ['alerts'])\ndfObj.head()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Empty! How many are there?"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "df.columns.size",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "One, should be multiple!\n\nGo back and try no 4."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "df45 = pd.read_table(io.BytesIO(streaming_body_4.read()))",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "df45.head()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "\nbody = client_589fdcec84424efeafef706c303c6778.get_object(Bucket='predlog-donotdelete-pr-eptzlrr64e0mwe',Key='outfile4.json')['Body']\n# add missing __iter__ method, so pandas accepts body as file-like object \n\nif not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n\n# Since JSON data can be semi-structured and contain additional metadata, it is possible that you might face an error during data loading.\n# Please read the documentation of 'pandas.read_json()' and 'pandas.io.json.json_normalize' to learn more about the possibilities to adjust the data loading.\n# pandas documentation: http://pandas.pydata.org/pandas-docs/stable/io.html#io-json-reader\n# and http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.json.json_normalize.html\n\ndf_data_3 = pd.read_json(body, orient='values')\ndf_data_3.head()\n",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# This is not right! Search blogs and posts on reading json. Found one on StringIO. Testing..."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "import json\nimport pandas as pd\n\nfrom pandas.io.json import json_normalize\n\nbody = client_589fdcec84424efeafef706c303c6778.get_object(Bucket='predlog-donotdelete-pr-eptzlrr64e0mwe',Key='outfile4.json')['Body']\n# add missing __iter__ method, so pandas accepts body as file-like object \n\nif not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n    \nstring_out = io.StringIO()\nstring_out.write(body.read().decode(\"utf-8\"))\n\nstring_out.getvalue()\n\n#data = json.load(open(string_out.getvalue()))\n#data = json.load(string_out.getvalue())\n\n#string_out.getvalue()\n#data = pd.read_json(string_out.getvalue())\n#df47 = pd.DataFrame(data[\"alerts\"])\n\n#df47.head()\n\njdata=json.loads(string_out.getvalue())\n#print(jdata['alerts'])\n\nlen(jdata['alerts'])",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# Wow! Now we're talking. This seems right. 2696 rows of data. Yes!"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "#for row in jdata['alerts']:\n#    \n#    print(row)\n    \n# credit to https://www.geeksforgeeks.org/pandas-parsing-json-dataset/\n\n\nnewdf = json_normalize(jdata['alerts']) \nnewdf.head(3) ",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# Bingo!"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "newdf.columns",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "newdf.columns.size",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "55 columns"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "newdf.shape",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "newdf",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "newdf.describe()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "newdf['category'].unique()\n",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "newdf['dataSource'].unique()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "newdf['type'].unique()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "newdf['name'].unique()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "#newdf['message'].unique() => 51\nnewdf['message']",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "newdf['message'][2690]",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# timestamp in column alertTime, try to convert to readable format"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "#df['date'] = pd.to_datetime(df['date'],unit='s')\n\nnewdf['alertTime'] = pd.to_datetime(newdf['alertTime'], unit='ms')",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "newdf.isnull().any()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "newdf.head(10)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Let's see all columns with data"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "pd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "newdf.head(10)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "newdf['links.clear.rel'].describe()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Hmm, still links.clear.rel and others show up as NaN in listing, still they have values (I guess string)."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "newdf.dtypes",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "newdf['links.clear.rel'] = newdf['links.clear.rel'].astype('str') ",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "newdf.dtypes",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Still object."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "newdf.head(10)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "newdf['links.clear.rel'][2687]",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "newdf.head()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "newdf.iloc[2687]",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "newdf.dtypes.value_counts() #normalize=True)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {
                "scrolled": false
            },
            "cell_type": "code",
            "source": "#df.astype({'col1': 'int32'}).dtypes\n\nnewdf.astype({'links.clear.rel': str})\n\nnewdf['links.clear.rel']= newdf['links.clear.rel'].astype(str) \n\nnewdf.dtypes.value_counts()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "newdf.head()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "newdf.dtypes",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# side track to test astype\n# importing pandas module  \nimport pandas as pd \n  \n# reading csv file from url  \ndata = pd.read_csv(\"https://media.geeksforgeeks.org/wp-content/uploads/nba.csv\") \n   \n# dropping null value columns to avoid errors \ndata.dropna(inplace = True) \n  \n# storing dtype before converting \nbefore = data.dtypes \n  \n# converting dtypes using astype \ndata[\"Salary\"]= data[\"Salary\"].astype(int) \ndata[\"Number\"]= data[\"Number\"].astype(str) \n  \n# storing dtype after converting \nafter = data.dtypes \n  \n# printing to compare \nprint(\"BEFORE CONVERSION\\n\", before, \"\\n\") \nprint(\"AFTER CONVERSION\\n\", after, \"\\n\") \n",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "newdf.head()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "newdf.isnull().any()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# Let's move on and save our work now to object storage."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# @hidden_cell\n# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n# You might want to remove those credentials before you share the notebook.\ncredentials_1 = {\n    'IAM_SERVICE_ID': '',\n    'IBM_API_KEY_ID': '',\n    'ENDPOINT': 'https://s3.eu-geo.objectstorage.service.networklayer.com',\n    'IBM_AUTH_ENDPOINT': 'https://iam.bluemix.net/oidc/token',\n    'BUCKET': '',\n    'FILE': 'pluto_18v2_1.csv'\n}\n\nclient_cred = ibm_boto3.client(service_name='s3',\nibm_api_key_id='SNpSnb-1SxG2oqVa2zuBjtNVRlHFxUM_iNcwoMx7Ejpy',\nibm_auth_endpoint=\"https://iam.eu-de.bluemix.net/oidc/token\",\nconfig=Config(signature_version='oauth'),\nendpoint_url='https://s3.eu-geo.objectstorage.service.networklayer.com')\n\nbucket='predlog-donotdelete-pr-eptzlrr64e0mwe'\n\n#{\n#  \"apikey\": \"SNpSnb-1SxG2oqVa2zuBjtNVRlHFxUM_iNcwoMx7Ejpy\",\n#  \"cos_hmac_keys\": {\n#    \"access_key_id\": \"a3e85bf6d2e34b70a493a800c700539b\",\n#    \"secret_access_key\": \"850abe2fb62cb91b03de71e8f532f31000d3bc277780937e\"\n#  },\n#  \"endpoints\": \"https://control.cloud-object-storage.cloud.ibm.com/v2/endpoints\",\n#  \"iam_apikey_description\": \"Auto-generated for key a3e85bf6-d2e3-4b70-a493-a800c700539b\",\n#  \"iam_apikey_name\": \"WDP-Editor-predlog-donotdelete-pr-eptzlrr64e0mwe\",\n#  \"iam_role_crn\": \"crn:v1:bluemix:public:iam::::serviceRole:Writer\",\n#  \"iam_serviceid_crn\": \"crn:v1:bluemix:public:iam-identity::a/45d319a4b25971b53b40137d095e14b0::serviceid:ServiceId-6daa41b5-c16d-47f1-b5af-113876a4899d\",\n#  \"resource_instance_id\": \"crn:v1:bluemix:public:cloud-object-storage:global:a/45d319a4b25971b53b40137d095e14b0:589fdcec-8442-4efe-afef-706c303c6778::\"\n#}",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Read back for test."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "\n#prepare files\nnewdf.to_pickle('./df_raw.pkl')\n\n#upload files to storage\nclient_cred.upload_file('./df_raw.pkl',bucket,'df_raw_cos.pkl')\n\n#Download file for data set\nclient_cred.download_file(Bucket=bucket,Key='df_raw_cos.pkl',Filename='./df_raw_local.pkl')\ndf_downloaded = pd.read_pickle('./df_raw_local.pkl')\n\ndf_downloaded.head()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# There you go. Prepared one file for futher analysis. \n\n# This concludes the initial data analysis on the log files."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "",
            "execution_count": null,
            "outputs": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.6",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.6.8",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}